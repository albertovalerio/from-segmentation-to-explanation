{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS Lesion Segmentation UNET Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform inference for an ensemble of models:\n",
    "* save 3D Nifti images of predicted probability maps averaged across ensemble models (saved to \"*pred_prob.nii.gz\" files), \n",
    "* binary segmentation maps predicted obtained by thresholding of average predictions and removing all connected components smaller than 9 voxels (saved to \"pred_seg.nii.gz\"), \n",
    "* uncertainty maps for reversed mutual information measure (saved to \"uncs_rmi.nii.gz\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-26T15:40:52.179463Z",
     "iopub.status.busy": "2024-04-26T15:40:52.178727Z",
     "iopub.status.idle": "2024-04-26T15:41:06.664818Z",
     "shell.execute_reply": "2024-04-26T15:41:06.663718Z",
     "shell.execute_reply.started": "2024-04-26T15:40:52.179431Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting monai==0.9.0\n",
      "  Downloading monai-0.9.0-202206131636-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.10/site-packages (from monai==0.9.0) (2.1.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from monai==0.9.0) (1.26.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.7->monai==0.9.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7->monai==0.9.0) (1.3.0)\n",
      "Downloading monai-0.9.0-202206131636-py3-none-any.whl (939 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.7/939.7 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: monai\n",
      "Successfully installed monai-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install monai==0.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-26T15:41:06.667046Z",
     "iopub.status.busy": "2024-04-26T15:41:06.666736Z",
     "iopub.status.idle": "2024-04-26T15:41:43.318501Z",
     "shell.execute_reply": "2024-04-26T15:41:43.317664Z",
     "shell.execute_reply.started": "2024-04-26T15:41:06.667017Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-26 15:41:35.252519: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-26 15:41:35.252624: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-26 15:41:35.384221: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.networks.nets import UNet\n",
    "from monai.data import write_nifti\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "from monai.transforms import (\n",
    "    AddChanneld, Compose, LoadImaged, RandCropByPosNegLabeld,\n",
    "    Spacingd, ToTensord, NormalizeIntensityd, RandFlipd,\n",
    "    RandRotate90d, RandShiftIntensityd, RandAffined, RandSpatialCropd,\n",
    "    RandScaleIntensityd)\n",
    "from scipy import ndimage\n",
    "#from data_load import remove_connected_components, get_flair_dataloader\n",
    "#from uncertainty import ensemble_uncertainties_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-26T15:41:43.320217Z",
     "iopub.status.busy": "2024-04-26T15:41:43.319602Z",
     "iopub.status.idle": "2024-04-26T15:41:43.325451Z",
     "shell.execute_reply": "2024-04-26T15:41:43.324287Z",
     "shell.execute_reply.started": "2024-04-26T15:41:43.320190Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\" Set device \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Got CUDA!\")\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-26T15:41:43.328829Z",
     "iopub.status.busy": "2024-04-26T15:41:43.328159Z",
     "iopub.status.idle": "2024-04-26T15:41:43.337000Z",
     "shell.execute_reply": "2024-04-26T15:41:43.336097Z",
     "shell.execute_reply.started": "2024-04-26T15:41:43.328803Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_val_transforms(keys=[\"image\", \"label\"], image_keys=[\"image\"]):\n",
    "    \"\"\" Get transforms for testing on FLAIR images and ground truth:\n",
    "    - Loads 3D images and masks from Nifti file\n",
    "    - Adds channel dimention\n",
    "    - Applies intensity normalisation to scans\n",
    "    - Converts to torch.Tensor()\n",
    "    \"\"\"\n",
    "    return Compose(\n",
    "        [\n",
    "            LoadImaged(keys=keys),\n",
    "            AddChanneld(keys=keys),\n",
    "            NormalizeIntensityd(keys=image_keys, nonzero=True),\n",
    "            ToTensord(keys=keys),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-26T15:41:43.338229Z",
     "iopub.status.busy": "2024-04-26T15:41:43.337980Z",
     "iopub.status.idle": "2024-04-26T15:41:43.348473Z",
     "shell.execute_reply": "2024-04-26T15:41:43.347699Z",
     "shell.execute_reply.started": "2024-04-26T15:41:43.338207Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_flair_dataloader(flair_path, num_workers, cache_rate=0.1, bm_path=None):\n",
    "    \"\"\"\n",
    "    Get dataloader with FLAIR images only for inference\n",
    "    \n",
    "    Args:\n",
    "      flair_path: `str`, path to directory with FLAIR images from Train set.\n",
    "      num_workers:  `int`,  number of worker threads to use for parallel processing\n",
    "                    of images\n",
    "      cache_rate:  `float` in (0.0, 1.0], percentage of cached data in total.\n",
    "      bm_path:   `None|str`. If `str`, then defines path to directory with\n",
    "                 brain masks. If `None`, dataloader does not return brain masks.\n",
    "    Returns:\n",
    "      monai.data.DataLoader() class object.\n",
    "    \"\"\"\n",
    "    flair = sorted(glob(os.path.join(flair_path, \"*FLAIR_isovox.nii\")),\n",
    "                   key=lambda i: int(re.sub('\\D', '', i)))  # Collect all flair images sorted\n",
    "\n",
    "    if bm_path is not None:\n",
    "        bms = sorted(glob(os.path.join(bm_path, \"*isovox_fg_mask.nii\")),\n",
    "                     key=lambda i: int(re.sub('\\D', '', i)))  # Collect all corresponding brain masks\n",
    "\n",
    "        assert len(flair) == len(bms), f\"Some files must be missing: {[len(flair), len(bms)]}\"\n",
    "\n",
    "        files = [{\"image\": fl, \"brain_mask\": bm} for fl, bm in zip(flair, bms)]\n",
    "\n",
    "        val_transforms = get_val_transforms(keys=[\"image\", \"brain_mask\"])\n",
    "    else:\n",
    "        files = [{\"image\": fl} for fl in flair]\n",
    "\n",
    "        val_transforms = get_val_transforms(keys=[\"image\"])\n",
    "\n",
    "    print(\"Number of FLAIR files:\", len(files))\n",
    "\n",
    "    ds = CacheDataset(data=files, transform=val_transforms,\n",
    "                      cache_rate=cache_rate, num_workers=num_workers)\n",
    "    return DataLoader(ds, batch_size=1, shuffle=False,\n",
    "                      num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-26T15:41:43.349818Z",
     "iopub.status.busy": "2024-04-26T15:41:43.349559Z",
     "iopub.status.idle": "2024-04-26T15:41:43.364101Z",
     "shell.execute_reply": "2024-04-26T15:41:43.363225Z",
     "shell.execute_reply.started": "2024-04-26T15:41:43.349796Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def remove_connected_components(segmentation, l_min=9):\n",
    "    \"\"\"\n",
    "    Remove all lesions with less or equal amount of voxels than `l_min` from a \n",
    "    binary segmentation mask `segmentation`.\n",
    "    Args:\n",
    "      segmentation: `numpy.ndarray` of shape [H, W, D], with a binary lesions segmentation mask.\n",
    "      l_min:  `int`, minimal amount of voxels in a lesion.\n",
    "    Returns:\n",
    "      Binary lesion segmentation mask (`numpy.ndarray` of shape [H, W, D])\n",
    "      only with connected components that have more than `l_min` voxels.\n",
    "    \"\"\"\n",
    "    labeled_seg, num_labels = ndimage.label(segmentation)\n",
    "    label_list = np.unique(labeled_seg)\n",
    "    num_elements_by_lesion = ndimage.labeled_comprehension(segmentation, labeled_seg, label_list, np.sum, float, 0)\n",
    "\n",
    "    seg2 = np.zeros_like(segmentation)\n",
    "    for i_el, n_el in enumerate(num_elements_by_lesion):\n",
    "        if n_el > l_min:\n",
    "            current_voxels = np.stack(np.where(labeled_seg == i_el), axis=1)\n",
    "            seg2[current_voxels[:, 0],\n",
    "                 current_voxels[:, 1],\n",
    "                 current_voxels[:, 2]] = 1\n",
    "    return seg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-26T15:41:43.365326Z",
     "iopub.status.busy": "2024-04-26T15:41:43.365037Z",
     "iopub.status.idle": "2024-04-26T15:41:43.380302Z",
     "shell.execute_reply": "2024-04-26T15:41:43.379520Z",
     "shell.execute_reply.started": "2024-04-26T15:41:43.365298Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def renyi_entropy_of_expected(probs, alpha=0.8):\n",
    "    \"\"\"\n",
    "    Renyi entropy is a generalised version of Shannon - the two are equivalent for alpha=1\n",
    "    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n",
    "    :return: array [num_voxels_X, num_voxels_Y, num_voxels_Z,]\n",
    "    \"\"\"\n",
    "    scale = 1. / (1. - alpha)\n",
    "    mean_probs = np.mean(probs, axis=0)\n",
    "    return scale * np.log( np.sum(mean_probs**alpha, axis=-1) )\n",
    "\n",
    "def renyi_expected_entropy(probs, alpha=0.8):\n",
    "    \"\"\"\n",
    "    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n",
    "    :return: array [num_voxels_X, num_voxels_Y, num_voxels_Z,]\n",
    "    \"\"\"\n",
    "    scale = 1. / (1. - alpha)\n",
    "    return np.mean( scale * np.log( np.sum(probs**alpha, axis=-1) ), axis=0)\n",
    "\n",
    "\n",
    "def entropy_of_expected(probs, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n",
    "    :return: array [num_voxels_X, num_voxels_Y, num_voxels_Z,]\n",
    "    \"\"\"\n",
    "    mean_probs = np.mean(probs, axis=0)\n",
    "    log_probs = -np.log(mean_probs + epsilon)\n",
    "    return np.sum(mean_probs * log_probs, axis=-1)\n",
    "\n",
    "def expected_entropy(probs, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n",
    "    :return: array [num_voxels_X, num_voxels_Y, num_voxels_Z,]\n",
    "    \"\"\"\n",
    "    log_probs = -np.log(probs + epsilon)\n",
    "    return np.mean(np.sum(probs * log_probs, axis=-1), axis=0)\n",
    "\n",
    "\n",
    "def ensemble_uncertainties_classification(probs, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n",
    "    :return: Dictionary of uncertainties\n",
    "    \"\"\"\n",
    "    mean_probs = np.mean(probs, axis=0)\n",
    "    mean_lprobs = np.mean(np.log(probs + epsilon), axis=0)\n",
    "    conf = np.max(mean_probs, axis=-1)\n",
    "\n",
    "    eoe = entropy_of_expected(probs, epsilon)\n",
    "    exe = expected_entropy(probs, epsilon)\n",
    "\n",
    "    mutual_info = eoe - exe\n",
    "\n",
    "    epkl = -np.sum(mean_probs * mean_lprobs, axis=-1) - exe\n",
    "\n",
    "    uncertainty = {'confidence': -1 * conf,\n",
    "                   'entropy_of_expected': eoe,\n",
    "                   'expected_entropy': exe,\n",
    "                   'mutual_information': mutual_info,\n",
    "                   'epkl': epkl,\n",
    "                   'reverse_mutual_information': epkl - mutual_info,\n",
    "                   }\n",
    "\n",
    "    return uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-26T15:57:31.819708Z",
     "iopub.status.busy": "2024-04-26T15:57:31.818826Z",
     "iopub.status.idle": "2024-04-26T15:57:31.840417Z",
     "shell.execute_reply": "2024-04-26T15:57:31.839354Z",
     "shell.execute_reply.started": "2024-04-26T15:57:31.819669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def inferenceUNET(path_pred, path_data, path_bm, threshold = 0.35, num_models = 3, path_model = '', num_workers = 1):\n",
    "    \n",
    "    #Setting up output directory\n",
    "    os.makedirs(path_pred, exist_ok=True)\n",
    "    \n",
    "    #Settin up device\n",
    "    device = get_default_device()\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "    \n",
    "    #Initialise dataloaders\n",
    "    val_loader = get_flair_dataloader(flair_path=path_data,\n",
    "                                      num_workers=num_workers,\n",
    "                                      bm_path=path_bm)\n",
    "    \n",
    "    #Load trained models\n",
    "    K = num_models\n",
    "    models = []\n",
    "    for i in range(K):\n",
    "        models.append(UNet(\n",
    "            spatial_dims=3,\n",
    "            in_channels=1,\n",
    "            out_channels=2,\n",
    "            channels=(32, 64, 128, 256, 512),\n",
    "            strides=(2, 2, 2, 2),\n",
    "            num_res_units=0).to(device))\n",
    "    \n",
    "    if(get_default_device() == torch.device('cpu')):\n",
    "        for i, model in enumerate(models):\n",
    "            model.load_state_dict(torch.load(os.path.join(path_model,\n",
    "                                                      f\"seed{i + 1}\",\n",
    "                                                      \"Best_model_finetuning.pth\"),\n",
    "                                                      map_location=torch.device('cpu'), weights_only=True))\n",
    "            model.eval()\n",
    "    else :\n",
    "        for i, model in enumerate(models):\n",
    "            model.load_state_dict(torch.load(os.path.join(path_model,\n",
    "                                                      f\"seed{i + 1}\",\n",
    "                                                      \"Best_model_finetuning.pth\"), weights_only=True))\n",
    "            model.eval()\n",
    "            \n",
    "    act = torch.nn.Softmax(dim=1)\n",
    "    th = threshold\n",
    "    roi_size = (96, 96, 96)\n",
    "    sw_batch_size = 4\n",
    "    \n",
    "    #Predictions loop\n",
    "    with torch.no_grad():\n",
    "        for count, batch_data in enumerate(val_loader):\n",
    "            inputs = batch_data[\"image\"].to(device)\n",
    "            foreground_mask = batch_data[\"brain_mask\"].numpy()[0, 0]\n",
    "\n",
    "            # get ensemble predictions\n",
    "            all_outputs = []\n",
    "            for model in models:\n",
    "                outputs = sliding_window_inference(inputs, roi_size, sw_batch_size, model, mode='gaussian')\n",
    "                outputs = act(outputs).cpu().numpy()\n",
    "                outputs = np.squeeze(outputs[0, 1])\n",
    "                all_outputs.append(outputs)\n",
    "            all_outputs = np.asarray(all_outputs)\n",
    "\n",
    "            # get image metadata\n",
    "            original_affine = batch_data['image_meta_dict']['original_affine'][0]\n",
    "            affine = batch_data['image_meta_dict']['affine'][0]\n",
    "            spatial_shape = batch_data['image_meta_dict']['spatial_shape'][0]\n",
    "            filename_or_obj = batch_data['image_meta_dict']['filename_or_obj'][0]\n",
    "            filename_or_obj = os.path.basename(filename_or_obj)\n",
    "\n",
    "            # obtain and save probability maps averaged across models in an ensemble\n",
    "            outputs_mean = np.mean(all_outputs, axis=0)\n",
    "\n",
    "            filename = re.sub(\"FLAIR_isovox.nii\", 'pred_prob.nii.gz',\n",
    "                              filename_or_obj)\n",
    "            filepath = os.path.join(path_pred, filename)\n",
    "            write_nifti(outputs_mean, filepath,\n",
    "                        affine=original_affine,\n",
    "                        target_affine=affine,\n",
    "                        output_spatial_shape=spatial_shape)\n",
    "\n",
    "            # obtain and save binary segmentation masks\n",
    "            seg = outputs_mean.copy()\n",
    "            seg[seg >= th] = 1\n",
    "            seg[seg < th] = 0\n",
    "            seg = np.squeeze(seg)\n",
    "            seg = remove_connected_components(seg)\n",
    "\n",
    "            filename = re.sub(\"FLAIR_isovox.nii\", 'pred_seg.nii.gz',\n",
    "                              filename_or_obj)\n",
    "            filepath = os.path.join(path_pred, filename)\n",
    "            write_nifti(seg, filepath,\n",
    "                        affine=original_affine,\n",
    "                        target_affine=affine,\n",
    "                        mode='nearest',\n",
    "                        output_spatial_shape=spatial_shape)\n",
    "\n",
    "            # obtain and save uncertainty map (voxel-wise reverse mutual information)\n",
    "            uncs_map = ensemble_uncertainties_classification(np.concatenate(\n",
    "                (np.expand_dims(all_outputs, axis=-1),\n",
    "                 np.expand_dims(1. - all_outputs, axis=-1)),\n",
    "                axis=-1))['reverse_mutual_information']\n",
    "\n",
    "            filename = re.sub(\"FLAIR_isovox.nii\", 'uncs_rmi.nii.gz',\n",
    "                              filename_or_obj)\n",
    "            filepath = os.path.join(path_pred, filename)\n",
    "            write_nifti(uncs_map * foreground_mask, filepath,\n",
    "                        affine=original_affine,\n",
    "                        target_affine=affine,\n",
    "                        output_spatial_shape=spatial_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-26T15:59:46.735977Z",
     "iopub.status.busy": "2024-04-26T15:59:46.735595Z",
     "iopub.status.idle": "2024-04-26T16:03:14.895121Z",
     "shell.execute_reply": "2024-04-26T16:03:14.893974Z",
     "shell.execute_reply.started": "2024-04-26T15:59:46.735948Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got CUDA!\n",
      "Number of FLAIR files: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 3/3 [00:00<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got CUDA!\n",
      "All Done!\n"
     ]
    }
   ],
   "source": [
    "path_pred = '/kaggle/working/predictions'\n",
    "path_data = \"/kaggle/input/sdcombinedextracted/ShiftsDatasetCombinedExtracted/Test/FLAIR\"\n",
    "path_bm = \"/kaggle/input/sdcombinedextracted/ShiftsDatasetCombinedExtracted/Test/FgMasks\"\n",
    "path_model = \"/kaggle/input/sdcombinedextracted/baselinetrained\"\n",
    "inferenceUNET(path_pred, path_data, path_bm, threshold = 0.35, num_models = 1, path_model = path_model, num_workers = 1)\n",
    "print(\"All Done!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4826829,
     "sourceId": 8166514,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
