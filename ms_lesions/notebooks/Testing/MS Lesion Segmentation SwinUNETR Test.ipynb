{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MS LESION SEGMENTATION TESTING SwinUNETR** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation of performance metrics (nDSC, lesion F1 score, nDSC R-AUC) for an ensemble of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T08:52:52.759937Z",
     "iopub.status.busy": "2024-04-29T08:52:52.759465Z",
     "iopub.status.idle": "2024-04-29T08:53:25.917657Z",
     "shell.execute_reply": "2024-04-29T08:53:25.916207Z",
     "shell.execute_reply.started": "2024-04-29T08:52:52.759892Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: monai==0.9.0 in d:\\user\\salvarki\\desktop\\github tesi project\\myenv\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: torch>=1.7 in d:\\user\\salvarki\\desktop\\github tesi project\\myenv\\lib\\site-packages (from monai==0.9.0) (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\user\\salvarki\\desktop\\github tesi project\\myenv\\lib\\site-packages (from monai==0.9.0) (1.26.4)\n",
      "Requirement already satisfied: filelock in d:\\user\\salvarki\\desktop\\github tesi project\\myenv\\lib\\site-packages (from torch>=1.7->monai==0.9.0) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\user\\salvarki\\desktop\\github tesi project\\myenv\\lib\\site-packages (from torch>=1.7->monai==0.9.0) (4.11.0)\n",
      "Requirement already satisfied: sympy in d:\\user\\salvarki\\desktop\\github tesi project\\myenv\\lib\\site-packages (from torch>=1.7->monai==0.9.0) (1.12.1)\n",
      "Requirement already satisfied: networkx in d:\\user\\salvarki\\desktop\\github tesi project\\myenv\\lib\\site-packages (from torch>=1.7->monai==0.9.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\user\\salvarki\\desktop\\github tesi project\\myenv\\lib\\site-packages (from torch>=1.7->monai==0.9.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\user\\salvarki\\desktop\\github tesi project\\myenv\\lib\\site-packages (from torch>=1.7->monai==0.9.0) (2024.6.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in d:\\user\\salvarki\\desktop\\github tesi project\\myenv\\lib\\site-packages (from torch>=1.7->monai==0.9.0) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in d:\\user\\salvarki\\desktop\\github tesi project\\myenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.7->monai==0.9.0) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in d:\\user\\salvarki\\desktop\\github tesi project\\myenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.7->monai==0.9.0) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\user\\salvarki\\desktop\\github tesi project\\myenv\\lib\\site-packages (from jinja2->torch>=1.7->monai==0.9.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in d:\\user\\salvarki\\desktop\\github tesi project\\myenv\\lib\\site-packages (from sympy->torch>=1.7->monai==0.9.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in d:\\user\\salvarki\\desktop\\github tesi project\\myenv\\lib\\site-packages (0.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install monai==0.9.0\n",
    "%pip install einops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T08:53:25.920541Z",
     "iopub.status.busy": "2024-04-29T08:53:25.920124Z",
     "iopub.status.idle": "2024-04-29T08:54:17.047536Z",
     "shell.execute_reply": "2024-04-29T08:54:17.046245Z",
     "shell.execute_reply.started": "2024-04-29T08:53:25.920504Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\User\\Salvarki\\Desktop\\GitHub Tesi Project\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from joblib import Parallel\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "from monai.transforms import (\n",
    "    AddChanneld, Compose, LoadImaged, RandCropByPosNegLabeld,\n",
    "    Spacingd, ToTensord, NormalizeIntensityd, RandFlipd,\n",
    "    RandRotate90d, RandShiftIntensityd, RandAffined, RandSpatialCropd,\n",
    "    RandScaleIntensityd)\n",
    "from glob import glob\n",
    "from scipy import ndimage\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T08:54:17.050316Z",
     "iopub.status.busy": "2024-04-29T08:54:17.049141Z",
     "iopub.status.idle": "2024-04-29T08:54:17.057290Z",
     "shell.execute_reply": "2024-04-29T08:54:17.056001Z",
     "shell.execute_reply.started": "2024-04-29T08:54:17.050273Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\" Set device \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Got CUDA!\")\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T08:54:17.061268Z",
     "iopub.status.busy": "2024-04-29T08:54:17.060755Z",
     "iopub.status.idle": "2024-04-29T08:54:17.099052Z",
     "shell.execute_reply": "2024-04-29T08:54:17.097801Z",
     "shell.execute_reply.started": "2024-04-29T08:54:17.061219Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def remove_connected_components(segmentation, l_min=9):\n",
    "    \"\"\"\n",
    "    Remove all lesions with less or equal amount of voxels than `l_min` from a \n",
    "    binary segmentation mask `segmentation`.\n",
    "    Args:\n",
    "      segmentation: `numpy.ndarray` of shape [H, W, D], with a binary lesions segmentation mask.\n",
    "      l_min:  `int`, minimal amount of voxels in a lesion.\n",
    "    Returns:\n",
    "      Binary lesion segmentation mask (`numpy.ndarray` of shape [H, W, D])\n",
    "      only with connected components that have more than `l_min` voxels.\n",
    "    \"\"\"\n",
    "    labeled_seg, num_labels = ndimage.label(segmentation)\n",
    "    label_list = np.unique(labeled_seg)\n",
    "    num_elements_by_lesion = ndimage.labeled_comprehension(segmentation, labeled_seg, label_list, np.sum, float, 0)\n",
    "\n",
    "    seg2 = np.zeros_like(segmentation)\n",
    "    for i_el, n_el in enumerate(num_elements_by_lesion):\n",
    "        if n_el > l_min:\n",
    "            current_voxels = np.stack(np.where(labeled_seg == i_el), axis=1)\n",
    "            seg2[current_voxels[:, 0],\n",
    "                 current_voxels[:, 1],\n",
    "                 current_voxels[:, 2]] = 1\n",
    "    return seg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T08:54:17.101139Z",
     "iopub.status.busy": "2024-04-29T08:54:17.100664Z",
     "iopub.status.idle": "2024-04-29T08:54:17.112295Z",
     "shell.execute_reply": "2024-04-29T08:54:17.111188Z",
     "shell.execute_reply.started": "2024-04-29T08:54:17.101098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_val_transforms(keys=[\"image\", \"label\"], image_keys=[\"image\"]):\n",
    "    \"\"\" Get transforms for testing on FLAIR images and ground truth:\n",
    "    - Loads 3D images and masks from Nifti file\n",
    "    - Adds channel dimention\n",
    "    - Applies intensity normalisation to scans\n",
    "    - Converts to torch.Tensor()\n",
    "    \"\"\"\n",
    "    return Compose(\n",
    "        [\n",
    "            LoadImaged(keys=keys),\n",
    "            AddChanneld(keys=keys),\n",
    "            NormalizeIntensityd(keys=image_keys, nonzero=True),\n",
    "            ToTensord(keys=keys),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T08:54:17.114198Z",
     "iopub.status.busy": "2024-04-29T08:54:17.113752Z",
     "iopub.status.idle": "2024-04-29T08:54:17.130054Z",
     "shell.execute_reply": "2024-04-29T08:54:17.128904Z",
     "shell.execute_reply.started": "2024-04-29T08:54:17.114159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_val_dataloader(flair_path, gts_path, num_workers, cache_rate=0.1, bm_path=None):\n",
    "    \"\"\"\n",
    "    Get dataloader for validation and testing. Either with or without brain masks.\n",
    "\n",
    "    Args:\n",
    "      flair_path: `str`, path to directory with FLAIR images.\n",
    "      gts_path:  `str`, path to directory with ground truth lesion segmentation \n",
    "                    binary masks images.\n",
    "      num_workers:  `int`,  number of worker threads to use for parallel processing\n",
    "                    of images\n",
    "      cache_rate:  `float` in (0.0, 1.0], percentage of cached data in total.\n",
    "      bm_path:   `None|str`. If `str`, then defines path to directory with\n",
    "                 brain masks. If `None`, dataloader does not return brain masks. \n",
    "    Returns:\n",
    "      monai.data.DataLoader() class object.\n",
    "    \"\"\"\n",
    "    flair = sorted(glob(os.path.join(flair_path, \"*FLAIR_isovox.nii\")),\n",
    "                   key=lambda i: int(re.sub('\\D', '', i)))  # Collect all flair images sorted\n",
    "    segs = sorted(glob(os.path.join(gts_path, \"*_isovox.nii\")),\n",
    "                  key=lambda i: int(re.sub('\\D', '', i)))  # Collect all corresponding ground truths\n",
    "\n",
    "    if bm_path is not None:\n",
    "        bms = sorted(glob(os.path.join(bm_path, \"*isovox_fg_mask.nii\")),\n",
    "                     key=lambda i: int(re.sub('\\D', '', i)))  # Collect all corresponding brain masks\n",
    "\n",
    "        assert len(flair) == len(segs) == len(bms), f\"Some files must be missing: {[len(flair), len(segs), len(bms)]}\"\n",
    "\n",
    "        files = [\n",
    "            {\"image\": fl, \"label\": seg, \"brain_mask\": bm} for fl, seg, bm\n",
    "            in zip(flair, segs, bms)\n",
    "        ]\n",
    "\n",
    "        val_transforms = get_val_transforms(keys=[\"image\", \"label\", \"brain_mask\"])\n",
    "    else:\n",
    "        assert len(flair) == len(segs), f\"Some files must be missing: {[len(flair), len(segs)]}\"\n",
    "\n",
    "        files = [{\"image\": fl, \"label\": seg} for fl, seg in zip(flair, segs)]\n",
    "\n",
    "        val_transforms = get_val_transforms()\n",
    "\n",
    "    print(\"Number of validation files:\", len(files))\n",
    "\n",
    "    ds = CacheDataset(data=files, transform=val_transforms,\n",
    "                      cache_rate=cache_rate, num_workers=num_workers)\n",
    "    return DataLoader(ds, batch_size=1, shuffle=False,\n",
    "                      num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T08:54:17.131759Z",
     "iopub.status.busy": "2024-04-29T08:54:17.131378Z",
     "iopub.status.idle": "2024-04-29T08:54:17.147050Z",
     "shell.execute_reply": "2024-04-29T08:54:17.145814Z",
     "shell.execute_reply.started": "2024-04-29T08:54:17.131729Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def dice_norm_metric(ground_truth, predictions):\n",
    "    \"\"\"\n",
    "    Compute Normalised Dice Coefficient (nDSC), \n",
    "    False positive rate (FPR),\n",
    "    False negative rate (FNR) for a single example.\n",
    "    \n",
    "    Args:\n",
    "      ground_truth: `numpy.ndarray`, binary ground truth segmentation target,\n",
    "                     with shape [H, W, D].\n",
    "      predictions:  `numpy.ndarray`, binary segmentation predictions,\n",
    "                     with shape [H, W, D].\n",
    "    Returns:\n",
    "      Normalised dice coefficient (`float` in [0.0, 1.0]),\n",
    "      False positive rate (`float` in [0.0, 1.0]),\n",
    "      False negative rate (`float` in [0.0, 1.0]),\n",
    "      between `ground_truth` and `predictions`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reference for normalized DSC\n",
    "    r = 0.001\n",
    "    # Cast to float32 type\n",
    "    gt = ground_truth.astype(\"float32\")\n",
    "    seg = predictions.astype(\"float32\")\n",
    "    im_sum = np.sum(seg) + np.sum(gt)\n",
    "    if im_sum == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        if np.sum(gt) == 0:\n",
    "            k = 1.0\n",
    "        else:\n",
    "            k = (1 - r) * np.sum(gt) / (r * (len(gt.flatten()) - np.sum(gt)))\n",
    "        tp = np.sum(seg[gt == 1])\n",
    "        fp = np.sum(seg[gt == 0])\n",
    "        fn = np.sum(gt[seg == 0])\n",
    "        fp_scaled = k * fp\n",
    "        dsc_norm = 2. * tp / (fp_scaled + 2. * tp + fn)\n",
    "        return dsc_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T08:54:17.149728Z",
     "iopub.status.busy": "2024-04-29T08:54:17.149263Z",
     "iopub.status.idle": "2024-04-29T08:54:17.161548Z",
     "shell.execute_reply": "2024-04-29T08:54:17.160175Z",
     "shell.execute_reply.started": "2024-04-29T08:54:17.149675Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def ndsc_aac_metric(ground_truth, predictions, uncertainties, parallel_backend=None):\n",
    "    \"\"\"\n",
    "    Compute area above Normalised Dice Coefficient (nDSC) retention curve for \n",
    "    one subject. `ground_truth`, `predictions`, `uncertainties` - are flattened \n",
    "    arrays of correponding 3D maps within the foreground mask only.\n",
    "    \n",
    "    Args:\n",
    "      ground_truth: `numpy.ndarray`, binary ground truth segmentation target,\n",
    "                     with shape [H * W * D]. \n",
    "      predictions:  `numpy.ndarray`, binary segmentation predictions,\n",
    "                     with shape [H * W * D].\n",
    "      uncertainties:  `numpy.ndarray`, voxel-wise uncertainties,\n",
    "                     with shape [H * W * D].\n",
    "      parallel_backend: `joblib.Parallel`, for parallel computation\n",
    "                     for different retention fractions.\n",
    "    Returns:\n",
    "      nDSC R-AAC (`float` in [0.0, 1.0]).\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_dice_norm(frac_, preds_, gts_, N_):\n",
    "        pos = int(N_ * frac_)\n",
    "        curr_preds = preds if pos == N_ else np.concatenate(\n",
    "            (preds_[:pos], gts_[pos:]))\n",
    "        return dice_norm_metric(gts_, curr_preds)\n",
    "\n",
    "    if parallel_backend is None:\n",
    "        parallel_backend = Parallel(n_jobs=1)\n",
    "\n",
    "    ordering = uncertainties.argsort()\n",
    "    gts = ground_truth[ordering].copy()\n",
    "    preds = predictions[ordering].copy()\n",
    "    N = len(gts)\n",
    "\n",
    "    # # Significant class imbalance means it is important to use logspacing between values\n",
    "    # # so that it is more granular for the higher retention fractions\n",
    "    fracs_retained = np.log(np.arange(200 + 1)[1:])\n",
    "    fracs_retained /= np.amax(fracs_retained)\n",
    "\n",
    "    process = partial(compute_dice_norm, preds_=preds, gts_=gts, N_=N)\n",
    "    dsc_norm_scores = np.asarray(\n",
    "        parallel_backend(delayed(process)(frac)\n",
    "                         for frac in fracs_retained)\n",
    "    )\n",
    "\n",
    "    return 1. - metrics.auc(fracs_retained, dsc_norm_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T08:54:17.163572Z",
     "iopub.status.busy": "2024-04-29T08:54:17.163193Z",
     "iopub.status.idle": "2024-04-29T08:54:17.179203Z",
     "shell.execute_reply": "2024-04-29T08:54:17.177851Z",
     "shell.execute_reply.started": "2024-04-29T08:54:17.163541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def intersection_over_union(mask1, mask2):\n",
    "    \"\"\"\n",
    "    Compute IoU for 2 binary masks.\n",
    "    \n",
    "    Args:\n",
    "      mask1: `numpy.ndarray`, binary mask.\n",
    "      mask2:  `numpy.ndarray`, binary mask of the same shape as `mask1`.\n",
    "    Returns:\n",
    "      Intersection over union between `mask1` and `mask2` (`float` in [0.0, 1.0]).\n",
    "    \"\"\"\n",
    "    return np.sum(mask1 * mask2) / np.sum(mask1 + mask2 - mask1 * mask2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T08:54:17.183961Z",
     "iopub.status.busy": "2024-04-29T08:54:17.183440Z",
     "iopub.status.idle": "2024-04-29T08:54:17.201455Z",
     "shell.execute_reply": "2024-04-29T08:54:17.200183Z",
     "shell.execute_reply.started": "2024-04-29T08:54:17.183916Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def lesion_f1_score(ground_truth, predictions, IoU_threshold=0.25, parallel_backend=None):\n",
    "    \"\"\"\n",
    "    Compute lesion-scale F1 score.\n",
    "    \n",
    "    Args:\n",
    "      ground_truth: `numpy.ndarray`, binary ground truth segmentation target,\n",
    "                     with shape [H, W, D].\n",
    "      predictions:  `numpy.ndarray`, binary segmentation predictions,\n",
    "                     with shape [H, W, D].\n",
    "      IoU_threshold: `float` in [0.0, 1.0], IoU threshold for max IoU between \n",
    "                     predicted and ground truth lesions to classify them as\n",
    "                     TP, FP or FN.\n",
    "      parallel_backend: `joblib.Parallel`, for parallel computation\n",
    "                     for different retention fractions.\n",
    "    Returns:\n",
    "      Intersection over union between `mask1` and `mask2` (`float` in [0.0, 1.0]).\n",
    "    \"\"\"\n",
    "\n",
    "    def get_tp_fp(label_pred, mask_multi_pred, mask_multi_gt):\n",
    "        mask_label_pred = (mask_multi_pred == label_pred).astype(int)\n",
    "        all_iou = [0.0]\n",
    "        # iterate only intersections\n",
    "        for int_label_gt in np.unique(mask_multi_gt * mask_label_pred):\n",
    "            if int_label_gt != 0.0:\n",
    "                mask_label_gt = (mask_multi_gt == int_label_gt).astype(int)\n",
    "                all_iou.append(intersection_over_union(\n",
    "                    mask_label_pred, mask_label_gt))\n",
    "        max_iou = max(all_iou)\n",
    "        if max_iou >= IoU_threshold:\n",
    "            return 'tp'\n",
    "        else:\n",
    "            return 'fp'\n",
    "\n",
    "    def get_fn(label_gt, mask_multi_pred, mask_multi_gt):\n",
    "        mask_label_gt = (mask_multi_gt == label_gt).astype(int)\n",
    "        all_iou = [0]\n",
    "        for int_label_pred in np.unique(mask_multi_pred * mask_label_gt):\n",
    "            if int_label_pred != 0.0:\n",
    "                mask_label_pred = (mask_multi_pred ==\n",
    "                                   int_label_pred).astype(int)\n",
    "                all_iou.append(intersection_over_union(\n",
    "                    mask_label_pred, mask_label_gt))\n",
    "        max_iou = max(all_iou)\n",
    "        if max_iou < IoU_threshold:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    mask_multi_pred_, n_les_pred = ndimage.label(predictions)\n",
    "    mask_multi_gt_, n_les_gt = ndimage.label(ground_truth)\n",
    "\n",
    "    if parallel_backend is None:\n",
    "        parallel_backend = Parallel(n_jobs=1)\n",
    "\n",
    "    process_fp_tp = partial(get_tp_fp, mask_multi_pred=mask_multi_pred_,\n",
    "                            mask_multi_gt=mask_multi_gt_)\n",
    "\n",
    "    tp_fp = parallel_backend(delayed(process_fp_tp)(label_pred)\n",
    "                             for label_pred in np.unique(mask_multi_pred_) if label_pred != 0)\n",
    "    counter = Counter(tp_fp)\n",
    "    tp = float(counter['tp'])\n",
    "    fp = float(counter['fp'])\n",
    "\n",
    "    process_fn = partial(get_fn, mask_multi_pred=mask_multi_pred_,\n",
    "                         mask_multi_gt=mask_multi_gt_)\n",
    "\n",
    "    fn = parallel_backend(delayed(process_fn)(label_gt)\n",
    "                          for label_gt in np.unique(mask_multi_gt_) if label_gt != 0)\n",
    "    fn = float(np.sum(fn))\n",
    "\n",
    "    f1 = 1.0 if tp + 0.5 * (fp + fn) == 0.0 else tp / (tp + 0.5 * (fp + fn))\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T08:54:17.203506Z",
     "iopub.status.busy": "2024-04-29T08:54:17.203044Z",
     "iopub.status.idle": "2024-04-29T08:54:17.220272Z",
     "shell.execute_reply": "2024-04-29T08:54:17.219219Z",
     "shell.execute_reply.started": "2024-04-29T08:54:17.203463Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def renyi_entropy_of_expected(probs, alpha=0.8):\n",
    "    \"\"\"\n",
    "    Renyi entropy is a generalised version of Shannon - the two are equivalent for alpha=1\n",
    "    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n",
    "    :return: array [num_voxels_X, num_voxels_Y, num_voxels_Z,]\n",
    "    \"\"\"\n",
    "    scale = 1. / (1. - alpha)\n",
    "    mean_probs = np.mean(probs, axis=0)\n",
    "    return scale * np.log( np.sum(mean_probs**alpha, axis=-1) )\n",
    "\n",
    "def renyi_expected_entropy(probs, alpha=0.8):\n",
    "    \"\"\"\n",
    "    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n",
    "    :return: array [num_voxels_X, num_voxels_Y, num_voxels_Z,]\n",
    "    \"\"\"\n",
    "    scale = 1. / (1. - alpha)\n",
    "    return np.mean( scale * np.log( np.sum(probs**alpha, axis=-1) ), axis=0)\n",
    "\n",
    "\n",
    "def entropy_of_expected(probs, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n",
    "    :return: array [num_voxels_X, num_voxels_Y, num_voxels_Z,]\n",
    "    \"\"\"\n",
    "    mean_probs = np.mean(probs, axis=0)\n",
    "    log_probs = -np.log(mean_probs + epsilon)\n",
    "    return np.sum(mean_probs * log_probs, axis=-1)\n",
    "\n",
    "def expected_entropy(probs, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n",
    "    :return: array [num_voxels_X, num_voxels_Y, num_voxels_Z,]\n",
    "    \"\"\"\n",
    "    log_probs = -np.log(probs + epsilon)\n",
    "    return np.mean(np.sum(probs * log_probs, axis=-1), axis=0)\n",
    "\n",
    "\n",
    "def ensemble_uncertainties_classification(probs, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n",
    "    :return: Dictionary of uncertainties\n",
    "    \"\"\"\n",
    "    mean_probs = np.mean(probs, axis=0)\n",
    "    mean_lprobs = np.mean(np.log(probs + epsilon), axis=0)\n",
    "    conf = np.max(mean_probs, axis=-1)\n",
    "\n",
    "    eoe = entropy_of_expected(probs, epsilon)\n",
    "    exe = expected_entropy(probs, epsilon)\n",
    "\n",
    "    mutual_info = eoe - exe\n",
    "\n",
    "    epkl = -np.sum(mean_probs * mean_lprobs, axis=-1) - exe\n",
    "\n",
    "    uncertainty = {'confidence': -1 * conf,\n",
    "                   'entropy_of_expected': eoe,\n",
    "                   'expected_entropy': exe,\n",
    "                   'mutual_information': mutual_info,\n",
    "                   'epkl': epkl,\n",
    "                   'reverse_mutual_information': epkl - mutual_info,\n",
    "                   }\n",
    "\n",
    "    return uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T08:54:17.222636Z",
     "iopub.status.busy": "2024-04-29T08:54:17.222156Z",
     "iopub.status.idle": "2024-04-29T08:54:17.249377Z",
     "shell.execute_reply": "2024-04-29T08:54:17.248182Z",
     "shell.execute_reply.started": "2024-04-29T08:54:17.222596Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing function for UNET model.\n",
    "Args:\n",
    "    Data Args:\n",
    "        path_test_data: `str`, path to directory with FLAIR images from test set.\n",
    "        path_test_gts: `str`, path to directory with ground truth lesion segmentation from test set.\n",
    "        path_test_bm: `str`, path to directory with brain masks from test set.\n",
    "    Model Args:\n",
    "        threshold: `float`, threshold for binary segmentation mask.\n",
    "        num_models: `int`, number of models to ensemble.\n",
    "        path_model: `str`, path to directory with trained models.\n",
    "        num_workers: `int`, number of worker threads to use for parallel processing of images.\n",
    "        n_jobs: `int`, number of jobs to run in parallel.\n",
    "\"\"\"\n",
    "def testSwinUNETR(path_test_data, path_test_gts, path_test_bm, threshold = 0.35, num_models = 3, path_model = '', num_workers = 1, n_jobs = 1):\n",
    "    #Setting up the device\n",
    "    device = get_default_device()\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "    \n",
    "    #Initialise dataloaders\n",
    "    val_loader = get_val_dataloader(flair_path=path_test_data,\n",
    "                                    gts_path=path_test_gts,\n",
    "                                    num_workers=num_workers ,\n",
    "                                    bm_path=path_test_bm)\n",
    "    # Load trained models\n",
    "    K = num_models\n",
    "    models = []\n",
    "    for i in range(K):\n",
    "        models.append(SwinUNETR(\n",
    "            img_size=(96,96,96),\n",
    "            spatial_dims=3,\n",
    "            in_channels=1,\n",
    "            out_channels=2).to(device))\n",
    "\n",
    "    if(get_default_device() == torch.device('cpu')):\n",
    "        for i, model in enumerate(models):\n",
    "            model.load_state_dict(torch.load(os.path.join(path_model,\n",
    "                                                      f\"Seed {i + 1}\",\n",
    "                                                      \"Best_model_finetuning_85_epoch.pth\"),\n",
    "                                                      map_location=torch.device('cpu'), weights_only=True))\n",
    "            model.eval()\n",
    "    else :\n",
    "        for i, model in enumerate(models):\n",
    "            model.load_state_dict(torch.load(os.path.join(path_model,\n",
    "                                                      f\"Seed {i + 1}\",\n",
    "                                                      \"Best_model_finetuning.pth\"), weights_only=True))\n",
    "            model.eval()\n",
    "    \n",
    "    act = torch.nn.Softmax(dim=1)\n",
    "    th = threshold\n",
    "    roi_size = (96, 96, 96)\n",
    "    sw_batch_size = 4\n",
    "\n",
    "    ndsc, f1, ndsc_aac = [], [], []\n",
    "\n",
    "    #Evaluatioin loop\n",
    "    with Parallel(n_jobs= n_jobs) as parallel_backend:\n",
    "        with torch.no_grad():\n",
    "            for count, batch_data in enumerate(val_loader):\n",
    "                inputs, gt, brain_mask = (\n",
    "                    batch_data[\"image\"].to(device),\n",
    "                    batch_data[\"label\"].cpu().numpy(),\n",
    "                    batch_data[\"brain_mask\"].cpu().numpy()\n",
    "                )\n",
    "\n",
    "                # get ensemble predictions\n",
    "                all_outputs = []\n",
    "                for model in models:\n",
    "                    outputs = sliding_window_inference(inputs, roi_size,\n",
    "                                                       sw_batch_size, model,\n",
    "                                                       mode='gaussian')\n",
    "                    outputs = act(outputs).cpu().numpy()\n",
    "                    outputs = np.squeeze(outputs[0, 1])\n",
    "                    all_outputs.append(outputs)\n",
    "                all_outputs = np.asarray(all_outputs)\n",
    "\n",
    "                # obtain binary segmentation mask\n",
    "                seg = np.mean(all_outputs, axis=0)\n",
    "                seg[seg >= th] = 1\n",
    "                seg[seg < th] = 0\n",
    "                seg = np.squeeze(seg)\n",
    "                seg = remove_connected_components(seg)\n",
    "\n",
    "                gt = np.squeeze(gt)\n",
    "                brain_mask = np.squeeze(brain_mask)\n",
    "\n",
    "                # compute reverse mutual information uncertainty map\n",
    "                uncs_map = ensemble_uncertainties_classification(np.concatenate(\n",
    "                    (np.expand_dims(all_outputs, axis=-1),\n",
    "                     np.expand_dims(1. - all_outputs, axis=-1)),\n",
    "                    axis=-1))['expected_entropy']\n",
    "\n",
    "                # compute metrics\n",
    "                ndsc += [dice_norm_metric(ground_truth=gt, predictions=seg)]\n",
    "                f1 += [lesion_f1_score(ground_truth=gt,\n",
    "                                       predictions=seg,\n",
    "                                       IoU_threshold=0.5,\n",
    "                                       parallel_backend=parallel_backend)]\n",
    "                ndsc_aac += [ndsc_aac_metric(ground_truth=gt[brain_mask == 1].flatten(),\n",
    "                                             predictions=seg[brain_mask == 1].flatten(),\n",
    "                                             uncertainties=uncs_map[brain_mask == 1].flatten(),\n",
    "                                             parallel_backend=parallel_backend)]\n",
    "\n",
    "                # for nervous people\n",
    "                if count % 10 == 0:\n",
    "                    print(f\"Processed {count}/{len(val_loader)}\")\n",
    "    \n",
    "    ndsc = np.asarray(ndsc) * 100.\n",
    "    f1 = np.asarray(f1) * 100.\n",
    "    ndsc_aac = np.asarray(ndsc_aac) * 100.\n",
    "\n",
    "    results = {\n",
    "        \"model_name\": \"SwinUNETR\",\n",
    "        \"nDSC\": np.mean(ndsc),\n",
    "        \"nDSC std\": np.std(ndsc),\n",
    "        \"Lesion F1 score\": np.mean(f1),\n",
    "        \"Lesion F1 score std\": np.std(f1),\n",
    "        \"nDSC R-AUC\": np.mean(ndsc_aac),\n",
    "        \"nDSC R-AUC std\": np.std(ndsc_aac)\n",
    "    }\n",
    "\n",
    "    print (results)\n",
    "    with open(\"SwinUNETR_results.json\", \"w\") as file:\n",
    "        json.dump(results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T08:54:17.251434Z",
     "iopub.status.busy": "2024-04-29T08:54:17.251057Z",
     "iopub.status.idle": "2024-04-29T08:58:18.458907Z",
     "shell.execute_reply": "2024-04-29T08:58:18.456515Z",
     "shell.execute_reply.started": "2024-04-29T08:54:17.251390Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation files: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 3/3 [00:00<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0/33\n",
      "Processed 10/33\n",
      "Processed 20/33\n",
      "Processed 30/33\n",
      "{'model_name': 'SwinUNETR', 'nDSC': 0.6755863936531131, 'nDSC std': 0.10211468111994419, 'Lesion F1 score': 0.326372408702275, 'Lesion F1 score std': 0.12483215027322078, 'nDSC R-AUC': 0.035294796165610376, 'nDSC R-AUC std': 0.029902136592531055}\n"
     ]
    }
   ],
   "source": [
    "path_test_data = 'D:/User/Salvarki/Desktop/Tesi WorkSpace/ShiftChallenge/Dataset/ShiftsDatasetCombinedExtracted/Test/FLAIR'\n",
    "path_test_gts = 'D:/User/Salvarki\\Desktop/Tesi WorkSpace/ShiftChallenge/Dataset/ShiftsDatasetCombinedExtracted/Test/GroundTruth'\n",
    "path_test_bm = 'D:/User/Salvarki\\Desktop/Tesi WorkSpace/ShiftChallenge/Dataset/ShiftsDatasetCombinedExtracted/Test/FgMasks'\n",
    "path_model = 'D:/User/Salvarki/Desktop/GitHub Tesi Project/Visual-and-Textual-Explainability-in-Brain-Multiple-Sclerosis-Detection-with-3D-MRI/models/SwinUNETR/'\n",
    "\n",
    "testSwinUNETR(path_test_data, path_test_gts, path_test_bm, threshold = 0.35, num_models = 1, path_model = path_model, num_workers = 4, n_jobs = 4)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4826829,
     "sourceId": 8166514,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4898872,
     "sourceId": 8255193,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
