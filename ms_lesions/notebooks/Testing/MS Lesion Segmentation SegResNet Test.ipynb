{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MS LESION SEGMENTATION TESTING SegResNet** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation of performance metrics (nDSC, lesion F1 score, nDSC R-AUC) for a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T15:16:41.767585Z",
     "iopub.status.busy": "2024-05-09T15:16:41.767119Z",
     "iopub.status.idle": "2024-05-09T15:16:58.820043Z",
     "shell.execute_reply": "2024-05-09T15:16:58.818296Z",
     "shell.execute_reply.started": "2024-05-09T15:16:41.767551Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting monai==0.9.0\n",
      "  Using cached monai-0.9.0-202206131636-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.10/site-packages (from monai==0.9.0) (2.1.2+cpu)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from monai==0.9.0) (1.26.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->monai==0.9.0) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.7->monai==0.9.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7->monai==0.9.0) (1.3.0)\n",
      "Using cached monai-0.9.0-202206131636-py3-none-any.whl (939 kB)\n",
      "Installing collected packages: monai\n",
      "Successfully installed monai-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install monai==0.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T15:16:58.823492Z",
     "iopub.status.busy": "2024-05-09T15:16:58.823032Z",
     "iopub.status.idle": "2024-05-09T15:17:46.730739Z",
     "shell.execute_reply": "2024-05-09T15:17:46.729340Z",
     "shell.execute_reply.started": "2024-05-09T15:16:58.823453Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 15:17:35.343384: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-09 15:17:35.343549: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-09 15:17:35.504972: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from joblib import Parallel\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.networks.nets import SegResNet\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "from monai.transforms import (\n",
    "    AddChanneld, Compose, LoadImaged, RandCropByPosNegLabeld,\n",
    "    Spacingd, ToTensord, NormalizeIntensityd, RandFlipd,\n",
    "    RandRotate90d, RandShiftIntensityd, RandAffined, RandSpatialCropd,\n",
    "    RandScaleIntensityd)\n",
    "from glob import glob\n",
    "from scipy import ndimage\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T15:17:46.734270Z",
     "iopub.status.busy": "2024-05-09T15:17:46.732889Z",
     "iopub.status.idle": "2024-05-09T15:17:46.742444Z",
     "shell.execute_reply": "2024-05-09T15:17:46.740629Z",
     "shell.execute_reply.started": "2024-05-09T15:17:46.734227Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\" Set device \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Got CUDA!\")\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T15:17:46.745785Z",
     "iopub.status.busy": "2024-05-09T15:17:46.745080Z",
     "iopub.status.idle": "2024-05-09T15:17:46.796209Z",
     "shell.execute_reply": "2024-05-09T15:17:46.794752Z",
     "shell.execute_reply.started": "2024-05-09T15:17:46.745743Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def remove_connected_components(segmentation, l_min=9):\n",
    "    \"\"\"\n",
    "    Remove all lesions with less or equal amount of voxels than `l_min` from a \n",
    "    binary segmentation mask `segmentation`.\n",
    "    Args:\n",
    "      segmentation: `numpy.ndarray` of shape [H, W, D], with a binary lesions segmentation mask.\n",
    "      l_min:  `int`, minimal amount of voxels in a lesion.\n",
    "    Returns:\n",
    "      Binary lesion segmentation mask (`numpy.ndarray` of shape [H, W, D])\n",
    "      only with connected components that have more than `l_min` voxels.\n",
    "    \"\"\"\n",
    "    labeled_seg, num_labels = ndimage.label(segmentation)\n",
    "    label_list = np.unique(labeled_seg)\n",
    "    num_elements_by_lesion = ndimage.labeled_comprehension(segmentation, labeled_seg, label_list, np.sum, float, 0)\n",
    "\n",
    "    seg2 = np.zeros_like(segmentation)\n",
    "    for i_el, n_el in enumerate(num_elements_by_lesion):\n",
    "        if n_el > l_min:\n",
    "            current_voxels = np.stack(np.where(labeled_seg == i_el), axis=1)\n",
    "            seg2[current_voxels[:, 0],\n",
    "                 current_voxels[:, 1],\n",
    "                 current_voxels[:, 2]] = 1\n",
    "    return seg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T15:17:46.801231Z",
     "iopub.status.busy": "2024-05-09T15:17:46.800022Z",
     "iopub.status.idle": "2024-05-09T15:17:46.814168Z",
     "shell.execute_reply": "2024-05-09T15:17:46.812956Z",
     "shell.execute_reply.started": "2024-05-09T15:17:46.801179Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_val_transforms(keys=[\"image\", \"label\"], image_keys=[\"image\"]):\n",
    "    \"\"\" Get transforms for testing on FLAIR images and ground truth:\n",
    "    - Loads 3D images and masks from Nifti file\n",
    "    - Adds channel dimention\n",
    "    - Applies intensity normalisation to scans\n",
    "    - Converts to torch.Tensor()\n",
    "    \"\"\"\n",
    "    return Compose(\n",
    "        [\n",
    "            LoadImaged(keys=keys),\n",
    "            AddChanneld(keys=keys),\n",
    "            NormalizeIntensityd(keys=image_keys, nonzero=True),\n",
    "            ToTensord(keys=keys),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T15:17:46.816948Z",
     "iopub.status.busy": "2024-05-09T15:17:46.816108Z",
     "iopub.status.idle": "2024-05-09T15:17:46.835411Z",
     "shell.execute_reply": "2024-05-09T15:17:46.834108Z",
     "shell.execute_reply.started": "2024-05-09T15:17:46.816909Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_val_dataloader(flair_path, gts_path, num_workers, cache_rate=0.1, bm_path=None):\n",
    "    \"\"\"\n",
    "    Get dataloader for validation and testing. Either with or without brain masks.\n",
    "\n",
    "    Args:\n",
    "      flair_path: `str`, path to directory with FLAIR images.\n",
    "      gts_path:  `str`, path to directory with ground truth lesion segmentation \n",
    "                    binary masks images.\n",
    "      num_workers:  `int`,  number of worker threads to use for parallel processing\n",
    "                    of images\n",
    "      cache_rate:  `float` in (0.0, 1.0], percentage of cached data in total.\n",
    "      bm_path:   `None|str`. If `str`, then defines path to directory with\n",
    "                 brain masks. If `None`, dataloader does not return brain masks. \n",
    "    Returns:\n",
    "      monai.data.DataLoader() class object.\n",
    "    \"\"\"\n",
    "    flair = sorted(glob(os.path.join(flair_path, \"*FLAIR_isovox.nii\")),\n",
    "                   key=lambda i: int(re.sub('\\D', '', i)))  # Collect all flair images sorted\n",
    "    segs = sorted(glob(os.path.join(gts_path, \"*_isovox.nii\")),\n",
    "                  key=lambda i: int(re.sub('\\D', '', i)))  # Collect all corresponding ground truths\n",
    "\n",
    "    if bm_path is not None:\n",
    "        bms = sorted(glob(os.path.join(bm_path, \"*isovox_fg_mask.nii\")),\n",
    "                     key=lambda i: int(re.sub('\\D', '', i)))  # Collect all corresponding brain masks\n",
    "\n",
    "        assert len(flair) == len(segs) == len(bms), f\"Some files must be missing: {[len(flair), len(segs), len(bms)]}\"\n",
    "\n",
    "        files = [\n",
    "            {\"image\": fl, \"label\": seg, \"brain_mask\": bm} for fl, seg, bm\n",
    "            in zip(flair, segs, bms)\n",
    "        ]\n",
    "\n",
    "        val_transforms = get_val_transforms(keys=[\"image\", \"label\", \"brain_mask\"])\n",
    "    else:\n",
    "        assert len(flair) == len(segs), f\"Some files must be missing: {[len(flair), len(segs)]}\"\n",
    "\n",
    "        files = [{\"image\": fl, \"label\": seg} for fl, seg in zip(flair, segs)]\n",
    "\n",
    "        val_transforms = get_val_transforms()\n",
    "\n",
    "    print(\"Number of validation files:\", len(files))\n",
    "\n",
    "    ds = CacheDataset(data=files, transform=val_transforms,\n",
    "                      cache_rate=cache_rate, num_workers=num_workers)\n",
    "    return DataLoader(ds, batch_size=1, shuffle=False,\n",
    "                      num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T15:17:46.838728Z",
     "iopub.status.busy": "2024-05-09T15:17:46.837778Z",
     "iopub.status.idle": "2024-05-09T15:17:46.857348Z",
     "shell.execute_reply": "2024-05-09T15:17:46.855660Z",
     "shell.execute_reply.started": "2024-05-09T15:17:46.838662Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def dice_norm_metric(ground_truth, predictions):\n",
    "    \"\"\"\n",
    "    Compute Normalised Dice Coefficient (nDSC), \n",
    "    False positive rate (FPR),\n",
    "    False negative rate (FNR) for a single example.\n",
    "    \n",
    "    Args:\n",
    "      ground_truth: `numpy.ndarray`, binary ground truth segmentation target,\n",
    "                     with shape [H, W, D].\n",
    "      predictions:  `numpy.ndarray`, binary segmentation predictions,\n",
    "                     with shape [H, W, D].\n",
    "    Returns:\n",
    "      Normalised dice coefficient (`float` in [0.0, 1.0]),\n",
    "      False positive rate (`float` in [0.0, 1.0]),\n",
    "      False negative rate (`float` in [0.0, 1.0]),\n",
    "      between `ground_truth` and `predictions`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reference for normalized DSC\n",
    "    r = 0.001\n",
    "    # Cast to float32 type\n",
    "    gt = ground_truth.astype(\"float32\")\n",
    "    seg = predictions.astype(\"float32\")\n",
    "    im_sum = np.sum(seg) + np.sum(gt)\n",
    "    if im_sum == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        if np.sum(gt) == 0:\n",
    "            k = 1.0\n",
    "        else:\n",
    "            k = (1 - r) * np.sum(gt) / (r * (len(gt.flatten()) - np.sum(gt)))\n",
    "        tp = np.sum(seg[gt == 1])\n",
    "        fp = np.sum(seg[gt == 0])\n",
    "        fn = np.sum(gt[seg == 0])\n",
    "        fp_scaled = k * fp\n",
    "        dsc_norm = 2. * tp / (fp_scaled + 2. * tp + fn)\n",
    "        return dsc_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T15:17:46.860289Z",
     "iopub.status.busy": "2024-05-09T15:17:46.859278Z",
     "iopub.status.idle": "2024-05-09T15:17:46.880707Z",
     "shell.execute_reply": "2024-05-09T15:17:46.879330Z",
     "shell.execute_reply.started": "2024-05-09T15:17:46.860250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def ndsc_aac_metric(ground_truth, predictions, uncertainties, parallel_backend=None):\n",
    "    \"\"\"\n",
    "    Compute area above Normalised Dice Coefficient (nDSC) retention curve for \n",
    "    one subject. `ground_truth`, `predictions`, `uncertainties` - are flattened \n",
    "    arrays of correponding 3D maps within the foreground mask only.\n",
    "    \n",
    "    Args:\n",
    "      ground_truth: `numpy.ndarray`, binary ground truth segmentation target,\n",
    "                     with shape [H * W * D]. \n",
    "      predictions:  `numpy.ndarray`, binary segmentation predictions,\n",
    "                     with shape [H * W * D].\n",
    "      uncertainties:  `numpy.ndarray`, voxel-wise uncertainties,\n",
    "                     with shape [H * W * D].\n",
    "      parallel_backend: `joblib.Parallel`, for parallel computation\n",
    "                     for different retention fractions.\n",
    "    Returns:\n",
    "      nDSC R-AAC (`float` in [0.0, 1.0]).\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_dice_norm(frac_, preds_, gts_, N_):\n",
    "        pos = int(N_ * frac_)\n",
    "        curr_preds = preds if pos == N_ else np.concatenate(\n",
    "            (preds_[:pos], gts_[pos:]))\n",
    "        return dice_norm_metric(gts_, curr_preds)\n",
    "\n",
    "    if parallel_backend is None:\n",
    "        parallel_backend = Parallel(n_jobs=1)\n",
    "\n",
    "    ordering = uncertainties.argsort()\n",
    "    gts = ground_truth[ordering].copy()\n",
    "    preds = predictions[ordering].copy()\n",
    "    N = len(gts)\n",
    "\n",
    "    # # Significant class imbalance means it is important to use logspacing between values\n",
    "    # # so that it is more granular for the higher retention fractions\n",
    "    fracs_retained = np.log(np.arange(200 + 1)[1:])\n",
    "    fracs_retained /= np.amax(fracs_retained)\n",
    "\n",
    "    process = partial(compute_dice_norm, preds_=preds, gts_=gts, N_=N)\n",
    "    dsc_norm_scores = np.asarray(\n",
    "        parallel_backend(delayed(process)(frac)\n",
    "                         for frac in fracs_retained)\n",
    "    )\n",
    "\n",
    "    return 1. - metrics.auc(fracs_retained, dsc_norm_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T15:17:46.884204Z",
     "iopub.status.busy": "2024-05-09T15:17:46.882761Z",
     "iopub.status.idle": "2024-05-09T15:17:46.902447Z",
     "shell.execute_reply": "2024-05-09T15:17:46.900610Z",
     "shell.execute_reply.started": "2024-05-09T15:17:46.884155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def intersection_over_union(mask1, mask2):\n",
    "    \"\"\"\n",
    "    Compute IoU for 2 binary masks.\n",
    "    \n",
    "    Args:\n",
    "      mask1: `numpy.ndarray`, binary mask.\n",
    "      mask2:  `numpy.ndarray`, binary mask of the same shape as `mask1`.\n",
    "    Returns:\n",
    "      Intersection over union between `mask1` and `mask2` (`float` in [0.0, 1.0]).\n",
    "    \"\"\"\n",
    "    return np.sum(mask1 * mask2) / np.sum(mask1 + mask2 - mask1 * mask2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T15:17:46.905074Z",
     "iopub.status.busy": "2024-05-09T15:17:46.904603Z",
     "iopub.status.idle": "2024-05-09T15:17:46.924188Z",
     "shell.execute_reply": "2024-05-09T15:17:46.922598Z",
     "shell.execute_reply.started": "2024-05-09T15:17:46.905039Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def lesion_f1_score(ground_truth, predictions, IoU_threshold=0.25, parallel_backend=None):\n",
    "    \"\"\"\n",
    "    Compute lesion-scale F1 score.\n",
    "    \n",
    "    Args:\n",
    "      ground_truth: `numpy.ndarray`, binary ground truth segmentation target,\n",
    "                     with shape [H, W, D].\n",
    "      predictions:  `numpy.ndarray`, binary segmentation predictions,\n",
    "                     with shape [H, W, D].\n",
    "      IoU_threshold: `float` in [0.0, 1.0], IoU threshold for max IoU between \n",
    "                     predicted and ground truth lesions to classify them as\n",
    "                     TP, FP or FN.\n",
    "      parallel_backend: `joblib.Parallel`, for parallel computation\n",
    "                     for different retention fractions.\n",
    "    Returns:\n",
    "      Intersection over union between `mask1` and `mask2` (`float` in [0.0, 1.0]).\n",
    "    \"\"\"\n",
    "\n",
    "    def get_tp_fp(label_pred, mask_multi_pred, mask_multi_gt):\n",
    "        mask_label_pred = (mask_multi_pred == label_pred).astype(int)\n",
    "        all_iou = [0.0]\n",
    "        # iterate only intersections\n",
    "        for int_label_gt in np.unique(mask_multi_gt * mask_label_pred):\n",
    "            if int_label_gt != 0.0:\n",
    "                mask_label_gt = (mask_multi_gt == int_label_gt).astype(int)\n",
    "                all_iou.append(intersection_over_union(\n",
    "                    mask_label_pred, mask_label_gt))\n",
    "        max_iou = max(all_iou)\n",
    "        if max_iou >= IoU_threshold:\n",
    "            return 'tp'\n",
    "        else:\n",
    "            return 'fp'\n",
    "\n",
    "    def get_fn(label_gt, mask_multi_pred, mask_multi_gt):\n",
    "        mask_label_gt = (mask_multi_gt == label_gt).astype(int)\n",
    "        all_iou = [0]\n",
    "        for int_label_pred in np.unique(mask_multi_pred * mask_label_gt):\n",
    "            if int_label_pred != 0.0:\n",
    "                mask_label_pred = (mask_multi_pred ==\n",
    "                                   int_label_pred).astype(int)\n",
    "                all_iou.append(intersection_over_union(\n",
    "                    mask_label_pred, mask_label_gt))\n",
    "        max_iou = max(all_iou)\n",
    "        if max_iou < IoU_threshold:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    mask_multi_pred_, n_les_pred = ndimage.label(predictions)\n",
    "    mask_multi_gt_, n_les_gt = ndimage.label(ground_truth)\n",
    "\n",
    "    if parallel_backend is None:\n",
    "        parallel_backend = Parallel(n_jobs=1)\n",
    "\n",
    "    process_fp_tp = partial(get_tp_fp, mask_multi_pred=mask_multi_pred_,\n",
    "                            mask_multi_gt=mask_multi_gt_)\n",
    "\n",
    "    tp_fp = parallel_backend(delayed(process_fp_tp)(label_pred)\n",
    "                             for label_pred in np.unique(mask_multi_pred_) if label_pred != 0)\n",
    "    counter = Counter(tp_fp)\n",
    "    tp = float(counter['tp'])\n",
    "    fp = float(counter['fp'])\n",
    "\n",
    "    process_fn = partial(get_fn, mask_multi_pred=mask_multi_pred_,\n",
    "                         mask_multi_gt=mask_multi_gt_)\n",
    "\n",
    "    fn = parallel_backend(delayed(process_fn)(label_gt)\n",
    "                          for label_gt in np.unique(mask_multi_gt_) if label_gt != 0)\n",
    "    fn = float(np.sum(fn))\n",
    "\n",
    "    f1 = 1.0 if tp + 0.5 * (fp + fn) == 0.0 else tp / (tp + 0.5 * (fp + fn))\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T15:17:46.925970Z",
     "iopub.status.busy": "2024-05-09T15:17:46.925540Z",
     "iopub.status.idle": "2024-05-09T15:17:46.948522Z",
     "shell.execute_reply": "2024-05-09T15:17:46.947294Z",
     "shell.execute_reply.started": "2024-05-09T15:17:46.925937Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def renyi_entropy_of_expected(probs, alpha=0.8):\n",
    "    \"\"\"\n",
    "    Renyi entropy is a generalised version of Shannon - the two are equivalent for alpha=1\n",
    "    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n",
    "    :return: array [num_voxels_X, num_voxels_Y, num_voxels_Z,]\n",
    "    \"\"\"\n",
    "    scale = 1. / (1. - alpha)\n",
    "    mean_probs = np.mean(probs, axis=0)\n",
    "    return scale * np.log( np.sum(mean_probs**alpha, axis=-1) )\n",
    "\n",
    "def renyi_expected_entropy(probs, alpha=0.8):\n",
    "    \"\"\"\n",
    "    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n",
    "    :return: array [num_voxels_X, num_voxels_Y, num_voxels_Z,]\n",
    "    \"\"\"\n",
    "    scale = 1. / (1. - alpha)\n",
    "    return np.mean( scale * np.log( np.sum(probs**alpha, axis=-1) ), axis=0)\n",
    "\n",
    "\n",
    "def entropy_of_expected(probs, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n",
    "    :return: array [num_voxels_X, num_voxels_Y, num_voxels_Z,]\n",
    "    \"\"\"\n",
    "    mean_probs = np.mean(probs, axis=0)\n",
    "    log_probs = -np.log(mean_probs + epsilon)\n",
    "    return np.sum(mean_probs * log_probs, axis=-1)\n",
    "\n",
    "def expected_entropy(probs, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n",
    "    :return: array [num_voxels_X, num_voxels_Y, num_voxels_Z,]\n",
    "    \"\"\"\n",
    "    log_probs = -np.log(probs + epsilon)\n",
    "    return np.mean(np.sum(probs * log_probs, axis=-1), axis=0)\n",
    "\n",
    "\n",
    "def ensemble_uncertainties_classification(probs, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    :param probs: array [num_models, num_voxels_X, num_voxels_Y, num_voxels_Z, num_classes]\n",
    "    :return: Dictionary of uncertainties\n",
    "    \"\"\"\n",
    "    mean_probs = np.mean(probs, axis=0)\n",
    "    mean_lprobs = np.mean(np.log(probs + epsilon), axis=0)\n",
    "    conf = np.max(mean_probs, axis=-1)\n",
    "\n",
    "    eoe = entropy_of_expected(probs, epsilon)\n",
    "    exe = expected_entropy(probs, epsilon)\n",
    "\n",
    "    mutual_info = eoe - exe\n",
    "\n",
    "    epkl = -np.sum(mean_probs * mean_lprobs, axis=-1) - exe\n",
    "\n",
    "    uncertainty = {'confidence': -1 * conf,\n",
    "                   'entropy_of_expected': eoe,\n",
    "                   'expected_entropy': exe,\n",
    "                   'mutual_information': mutual_info,\n",
    "                   'epkl': epkl,\n",
    "                   'reverse_mutual_information': epkl - mutual_info,\n",
    "                   }\n",
    "\n",
    "    return uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T15:17:46.950703Z",
     "iopub.status.busy": "2024-05-09T15:17:46.950193Z",
     "iopub.status.idle": "2024-05-09T15:17:46.978577Z",
     "shell.execute_reply": "2024-05-09T15:17:46.977125Z",
     "shell.execute_reply.started": "2024-05-09T15:17:46.950671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing function for UNET model.\n",
    "Args:\n",
    "    Data Args:\n",
    "        path_test_data: `str`, path to directory with FLAIR images from test set.\n",
    "        path_test_gts: `str`, path to directory with ground truth lesion segmentation from test set.\n",
    "        path_test_bm: `str`, path to directory with brain masks from test set.\n",
    "    Model Args:\n",
    "        threshold: `float`, threshold for binary segmentation mask.\n",
    "        num_models: `int`, number of models to ensemble.\n",
    "        path_model: `str`, path to directory with trained models.\n",
    "        num_workers: `int`, number of worker threads to use for parallel processing of images.\n",
    "        n_jobs: `int`, number of jobs to run in parallel.\n",
    "\"\"\"\n",
    "def testSegResNet(path_test_data, path_test_gts, path_test_bm, threshold = 0.35, num_models = 3, path_model = '', num_workers = 1, n_jobs = 1):\n",
    "    #Setting up the device\n",
    "    device = get_default_device()\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "    \n",
    "    #Initialise dataloaders\n",
    "    val_loader = get_val_dataloader(flair_path=path_test_data,\n",
    "                                    gts_path=path_test_gts,\n",
    "                                    num_workers=num_workers ,\n",
    "                                    bm_path=path_test_bm)\n",
    "    # Load trained models\n",
    "    K = num_models\n",
    "    models = []\n",
    "    for i in range(K):\n",
    "        models.append(SegResNet(\n",
    "            spatial_dims=3,\n",
    "            in_channels=1,\n",
    "            out_channels=2).to(device))\n",
    "\n",
    "    if(get_default_device() == torch.device('cpu')):\n",
    "        for i, model in enumerate(models):\n",
    "            model.load_state_dict(torch.load(os.path.join(path_model,\n",
    "                                                      \"Best_model_finetuning.pth\"),\n",
    "                                                      map_location=torch.device('cpu'), weights_only=True))\n",
    "            model.eval()\n",
    "    else :\n",
    "        for i, model in enumerate(models):\n",
    "            model.load_state_dict(torch.load(os.path.join(path_model,\n",
    "                                                      \"Best_model_finetuning.pth\"), weights_only=True))\n",
    "            model.eval()\n",
    "    \n",
    "    act = torch.nn.Softmax(dim=1)\n",
    "    th = threshold\n",
    "    roi_size = (96, 96, 96)\n",
    "    sw_batch_size = 4\n",
    "\n",
    "    ndsc, f1, ndsc_aac = [], [], []\n",
    "\n",
    "    #Evaluatioin loop\n",
    "    with Parallel(n_jobs= n_jobs) as parallel_backend:\n",
    "        with torch.no_grad():\n",
    "            for count, batch_data in enumerate(val_loader):\n",
    "                inputs, gt, brain_mask = (\n",
    "                    batch_data[\"image\"].to(device),\n",
    "                    batch_data[\"label\"].cpu().numpy(),\n",
    "                    batch_data[\"brain_mask\"].cpu().numpy()\n",
    "                )\n",
    "\n",
    "                # get ensemble predictions\n",
    "                all_outputs = []\n",
    "                for model in models:\n",
    "                    outputs = sliding_window_inference(inputs, roi_size,\n",
    "                                                       sw_batch_size, model,\n",
    "                                                       mode='gaussian')\n",
    "                    outputs = act(outputs).cpu().numpy()\n",
    "                    outputs = np.squeeze(outputs[0, 1])\n",
    "                    all_outputs.append(outputs)\n",
    "                all_outputs = np.asarray(all_outputs)\n",
    "\n",
    "                # obtain binary segmentation mask\n",
    "                seg = np.mean(all_outputs, axis=0)\n",
    "                seg[seg >= th] = 1\n",
    "                seg[seg < th] = 0\n",
    "                seg = np.squeeze(seg)\n",
    "                seg = remove_connected_components(seg)\n",
    "\n",
    "                gt = np.squeeze(gt)\n",
    "                brain_mask = np.squeeze(brain_mask)\n",
    "\n",
    "                # compute reverse mutual information uncertainty map\n",
    "                uncs_map = ensemble_uncertainties_classification(np.concatenate(\n",
    "                    (np.expand_dims(all_outputs, axis=-1),\n",
    "                     np.expand_dims(1. - all_outputs, axis=-1)),\n",
    "                    axis=-1))['expected_entropy']\n",
    "\n",
    "                # compute metrics\n",
    "                ndsc += [dice_norm_metric(ground_truth=gt, predictions=seg)]\n",
    "                f1 += [lesion_f1_score(ground_truth=gt,\n",
    "                                       predictions=seg,\n",
    "                                       IoU_threshold=0.5,\n",
    "                                       parallel_backend=parallel_backend)]\n",
    "                ndsc_aac += [ndsc_aac_metric(ground_truth=gt[brain_mask == 1].flatten(),\n",
    "                                             predictions=seg[brain_mask == 1].flatten(),\n",
    "                                             uncertainties=uncs_map[brain_mask == 1].flatten(),\n",
    "                                             parallel_backend=parallel_backend)]\n",
    "\n",
    "                # for nervous people\n",
    "                if count % 10 == 0:\n",
    "                    print(f\"Processed {count}/{len(val_loader)}\")\n",
    "\n",
    "    ndsc = np.asarray(ndsc) * 100.\n",
    "    f1 = np.asarray(f1) * 100.\n",
    "    ndsc_aac = np.asarray(ndsc_aac) * 100.\n",
    "    results = {}\n",
    "    print(f\"nDSC:\\t{np.mean(ndsc):.4f} +- {np.std(ndsc):.4f}\")\n",
    "    print(f\"Lesion F1 score:\\t{np.mean(f1):.4f} +- {np.std(f1):.4f}\")\n",
    "    print(f\"nDSC R-AUC:\\t{np.mean(ndsc_aac):.4f} +- {np.std(ndsc_aac):.4f}\")\n",
    "    \n",
    "    results = {\n",
    "        \"model_name: \" = \"SegResNet\"\n",
    "        \"nDSC\": np.mean(ndsc),\n",
    "        \"nDSC std\": np.std(ndsc),\n",
    "        \"Lesion F1 score\": np.mean(f1),\n",
    "        \"Lesion F1 score std\": np.std(f1),\n",
    "        \"nDSC R-AUC\": np.mean(ndsc_aac),\n",
    "        \"nDSC R-AUC std\": np.std(ndsc_aac)\n",
    "    }\n",
    "\n",
    "    with open(\"/kaggle/working/SegResNet_results.json\", \"w\") as file:\n",
    "        json.dump(results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T15:17:46.980266Z",
     "iopub.status.busy": "2024-05-09T15:17:46.979856Z",
     "iopub.status.idle": "2024-05-09T15:49:31.923780Z",
     "shell.execute_reply": "2024-05-09T15:49:31.921584Z",
     "shell.execute_reply.started": "2024-05-09T15:17:46.980234Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation files: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 3/3 [00:01<00:00,  1.60it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid = os.fork()\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0/33\n",
      "Processed 10/33\n",
      "Processed 20/33\n",
      "Processed 30/33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDSC:\t71.7560 +- 8.6960\n",
      "Lesion F1 score:\t35.1109 +- 14.2631\n",
      "nDSC R-AUC:\t8.7450 +- 8.1628\n"
     ]
    }
   ],
   "source": [
    "path_test_data = '/kaggle/input/sdcombinedextracted/ShiftsDatasetCombinedExtracted/Test/FLAIR'\n",
    "path_test_gts = '/kaggle/input/sdcombinedextracted/ShiftsDatasetCombinedExtracted/Test/GroundTruth'\n",
    "path_test_bm = '/kaggle/input/sdcombinedextracted/ShiftsDatasetCombinedExtracted/Test/FgMasks'\n",
    "path_model = '/kaggle/input/segresnet/SegResNet'\n",
    "\n",
    "testSegResNet(path_test_data, path_test_gts, path_test_bm, threshold = 0.35, num_models = 1, path_model = path_model, num_workers = 4, n_jobs = -1)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4826829,
     "sourceId": 8166514,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4904105,
     "sourceId": 8262224,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
